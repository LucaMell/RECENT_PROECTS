{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6c0a7bc9e599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwin32com\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwin32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "from exchangelib import DELEGATE, Account, Credentials, FileAttachment, EWSDateTime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "import win32com.client as win32\n",
    "from win32com.client import constants\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from pdf2image import convert_from_path\n",
    "from pytz import timezone\n",
    "import transformers\n",
    "from transformers import pipeline, BertTokenizer,BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sqlalchemy import create_engine\n",
    "#import pymysql\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "#from prettytable import PrettyTable\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "import transformers\n",
    "from transformers import pipeline, BertTokenizer,BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sqlalchemy import create_engine\n",
    "#import pymysql\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from stdnum.dk import cvr\n",
    "import datefinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting da-core-news-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/da_core_news_lg-3.0.0/da_core_news_lg-3.0.0-py3-none-any.whl (574.5 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from da-core-news-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (4.60.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->da-core-news-lg==3.0.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('da_core_news_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "2021-09-06 09:19:09.804240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-09-06 09:19:09.804691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nb-core-news-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.0.0/nb_core_news_sm-3.0.0-py3-none-any.whl (16.2 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from nb-core-news-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (4.60.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->nb-core-news-sm==3.0.0) (1.1.1)\n",
      "Installing collected packages: nb-core-news-sm\n",
      "Successfully installed nb-core-news-sm-3.0.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('nb_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\dk2350\\Anaconda3\\envs\\demo\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "2021-09-06 09:19:46.651248: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-09-06 09:19:46.651548: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\dk2350\\anaconda3\\envs\\demo\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download da_core_news_lg\n",
    "!python -m spacy download nb_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Driver_emails:\n",
    "    def pdf_png_text(self,filename):\n",
    "        images = convert_from_path(filename, poppler_path=r'C:\\poppler-21.03.0\\Library\\bin')\n",
    "        text=''\n",
    "        for image in images:\n",
    "            try:\n",
    "                rotation = list(pytesseract.image_to_osd(image, output_type=Output.DICT).values())[1]\n",
    "                if rotation != 0:        \n",
    "                    image = image.rotate(rotation, expand=True)\n",
    "                text = text + pytesseract.image_to_string(image, lang = 'eng+dan').replace('\\n', ' ')\n",
    "            except:\n",
    "                pass\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def save_as_docx(self,path):\n",
    "        # Opening MS Word\n",
    "        word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "        doc = word.Documents.Open(path)\n",
    "        doc.Activate ()\n",
    "        # Rename path with .docx\n",
    "        new_file_abs = path + 'x'\n",
    "    \n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "            new_file_abs, FileFormat=constants.wdFormatXMLDocument\n",
    "        )\n",
    "        doc.Close(False)\n",
    "        os.remove(path)    \n",
    "    \n",
    "    def get_att_text(self,att_list):\n",
    "        attachment_text = '' # attachment_text er samlet tekst i alle vedhæftninger\n",
    "        # først håndteres pdf filer. Vi antager at pdf'en er tekstbaseret som det første\n",
    "        pdf_list=[item for item in att_list if item[-3:] in ['pdf', 'PDF']]    \n",
    "        for j in range(len(pdf_list)):\n",
    "            att_text='' # att_text er tekst i en enkelt tekstbaseret pdf fil \n",
    "            file = open(pdf_list[j], 'rb')\n",
    "            fileReader = PyPDF2.PdfFileReader(file)\n",
    "            try:\n",
    "                for k in range(fileReader.numPages):\n",
    "                    pageObj = fileReader.getPage(k)\n",
    "                    text = pageObj.extractText().replace('\\n', ' ').replace('\\r', ' ')\n",
    "                    att_text = att_text + ' ' + ' '.join(text.split())\n",
    "            except:\n",
    "                pass\n",
    "            file.close()\n",
    "            # hvis teksten er kortere end 20 tegn er filen formentlig billedbaseret\n",
    "            if len(att_text)<20:\n",
    "                att_text_ocr=self.pdf_png_text(pdf_list[j])\n",
    "            else:\n",
    "                att_text_ocr=''\n",
    "            if len(att_text_ocr)>len(att_text):\n",
    "                att_text=att_text_ocr\n",
    "            attachment_text = attachment_text + ' ' + att_text\n",
    "            \n",
    "        for f in pdf_list:\n",
    "            os.remove(f)\n",
    "            \n",
    "        # dernæst doc og docx filer\n",
    "        # først konverteres doc filer til docx filer\n",
    "        doc_list=[item for item in att_list if item[-3:]==\"doc\"]  \n",
    "        for item in doc_list:\n",
    "            self.save_as_docx('E:/Attachments/' + item)\n",
    "            att_list.append(item + 'x')\n",
    "        docx_list=[item for item in att_list if item[-4:]==\"docx\"]      \n",
    "        for j in range(len(docx_list)):\n",
    "            text = docx2txt.process(docx_list[j]).replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "            attachment_text = attachment_text + ' ' + ' '.join(text.split())\n",
    "        for f in docx_list:\n",
    "           os.remove(f)\n",
    "            \n",
    "        return(attachment_text[:50000])\n",
    "    \n",
    "    def pull_emails(self):\n",
    "        os.chdir('E:/Attachments')\n",
    "        credentials = Credentials(username='******', password='******')\n",
    "    \n",
    "        account = Account(\n",
    "          primary_smtp_address='*******', credentials=credentials,\n",
    "          autodiscover=True, access_type=DELEGATE\n",
    "        )\n",
    "    \n",
    "        sqlEngine = create_engine('mysql+pymysql://Doc_user:Doc_user@127.0.0.1/mailanalyse', pool_recycle=3600)\n",
    "        dbConnection = sqlEngine.connect()\n",
    "    \n",
    "        # Filtrer på mails efter seneste dato i db\n",
    "        UTC = timezone('UTC')\n",
    "        max_date=pd.read_sql('SELECT max(Received_UTC) FROM mailanalyse.indbakke_fuld', dbConnection)\n",
    "        max_date=UTC.localize(max_date['max(Received_UTC)'][0]).to_pydatetime()\n",
    "    \n",
    "        mail_list=[item for item in account.inbox.all().filter(datetime_received__gt=max_date).order_by('datetime_received')]\n",
    "    \n",
    "        sender_list=[] # afsender mail adresse\n",
    "        afsender_list=[] # navn på afsender i de tilfælde hvor mailen kommer fra e-boks\n",
    "        subject_list=[] # emne felt\n",
    "        body_list=[] # brødtekst\n",
    "        datetime_UTC_list=[] # mailserver kører på UTC tid\n",
    "        datetime_list=[] # tisdspunkt for modtagelsen\n",
    "        attachment_list=[] # liste af pdf, doc og docx vedhæftninger\n",
    "        attachment_text=[] # samlet tekst i alle vedhæftningerne \n",
    "        message_id_list=[]\n",
    "        references_list=[]\n",
    "        num_attachments_list=[]\n",
    "    \n",
    "        for i in range(len(mail_list)):\n",
    "    \n",
    "            sender_list.append(mail_list[i].sender.email_address)\n",
    "            subject_list.append(mail_list[i].subject)\n",
    "            message_id_list.append(mail_list[i].message_id)\n",
    "            references_list.append(mail_list[i].references)\n",
    "            try:\n",
    "                text=BeautifulSoup(mail_list[i].body).get_text()\n",
    "                text=' '.join(text.replace('\\n', ' ').replace('\\r', ' ').split())\n",
    "                body_list.append(text)\n",
    "            except:\n",
    "                body_list.append(' ')\n",
    "            if mail_list[i].sender.email_address=='******':\n",
    "                # navet på afsender står mellem 'Afsender:' og 'Referencenumre'\n",
    "                snippet=text[text.find('Afsender:')+10:text.find('Referencenumre:')-1]\n",
    "                if snippet.startswith('strong'):\n",
    "                    # i nogle tilfælde står der 'strong' før og efter afsenderen\n",
    "                    snippet=snippet[7:-10]\n",
    "                afsender_list.append(snippet)\n",
    "            else:\n",
    "                afsender_list.append(' ')\n",
    "            datetime_UTC_list.append(mail_list[i].datetime_received)\n",
    "            # exchange returnerer timestamps i UTC, derfor korrigeres med  1 eller 2 timer afhængig af sommertid\n",
    "            if int((datetime.now()-datetime.utcnow()).total_seconds())>4000:\n",
    "                diff=2\n",
    "            else:\n",
    "                diff=1\n",
    "            datetime_list.append(mail_list[i].datetime_received + timedelta(hours=diff))\n",
    "            att_name=[]\n",
    "            for j in range(len(mail_list[i].attachments)):\n",
    "                if isinstance(mail_list[i].attachments[j], FileAttachment):\n",
    "                    # check at vedhæftningen er en fil\n",
    "                    att_name.append(mail_list[i].attachments[j].name)\n",
    "                    extension=mail_list[i].attachments[j].name[-3:]\n",
    "                    if extension in ['pdf', 'PDF', 'doc', 'DOC', 'ocx', 'OCX']:\n",
    "                        # kun filer af den rigtige type gemmes på disk\n",
    "                        local_path = os.path.join('E:/Attachments', mail_list[i].attachments[j].name)\n",
    "                        with open(local_path, 'wb') as f:\n",
    "                            f.write(mail_list[i].attachments[j].content)\n",
    "            att_name=[item for item in att_name if item[-3:] in ['pdf', 'PDF', 'doc', 'DOC', 'ocx', 'OCX']]\n",
    "            attachment_list.append(att_name)\n",
    "            num_attachments_list.append(len(att_name))\n",
    "            attachment_text.append(self.get_att_text(att_name))\n",
    "    \n",
    "        data={'Sender': sender_list, 'Afsender': afsender_list, 'Subject': subject_list,\n",
    "              'Body': body_list, 'Received_UTC': datetime_UTC_list, 'Received': datetime_list, 'Attachments': attachment_list,\n",
    "              'Attachment_text': attachment_text, 'Num_attachments': num_attachments_list, 'Message_ID': message_id_list,\n",
    "              'References': references_list}\n",
    "    \n",
    "        df=pd.DataFrame.from_dict(data)\n",
    "        df=df.drop('Attachments', axis=1)\n",
    "    \n",
    "        tableName='indbakke_fuld'\n",
    "    \n",
    "        try:\n",
    "            frame = df.to_sql(tableName, dbConnection, if_exists='append', index=False);\n",
    "        except ValueError as vx:\n",
    "            print(vx)\n",
    "        except Exception as ex:   \n",
    "            #print(ex)\n",
    "        else:\n",
    "            print(\"Table %s created successfully.\"%tableName);   \n",
    "        finally:\n",
    "            dbConnection.close()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_inference:\n",
    "    def documentClassification(self,test_data=Driver_emails().pull_emails(),content_column='Body' ,model_path = 'E:/Scripts/custom ner/Master code/models_foa_5_labels', tokenizer_path = \"DJSammy/bert-base-danish-uncased_BotXO-ai\" , number_of_words = 200 , sql_server_name = 'localhost' , login = 'mysql+pymysql://Doc_user:Doc_user@127.0.0.1/documentintelligence', tableName='kptimes_classification'):\n",
    "        # Model and tokenizer loaded\n",
    "        model=BertForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        # Pipeline created\n",
    "        nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer = tokenizer, device=-1)\n",
    "        # Data changed to lists\n",
    "        test_list=test_data[content_column].values.tolist()\n",
    "        pred_list=[]\n",
    "        score_list=[]\n",
    "        content=[]\n",
    "        # Document classification is performed on the input documents\n",
    "        for i in tqdm(range(len(test_list))):\n",
    "    \t      # truncate to first 200 woords\n",
    "            temp=test_list[i][0:number_of_words]\n",
    "            result=nlp(temp)[0]\n",
    "            pred_list.append(result['label'])\n",
    "            score_list.append(result['score'])\n",
    "            content.append(temp)\n",
    "        # Documents indexed properly(?)    \n",
    "        pred_num_list=[int(item[-1]) for item in pred_list]\n",
    "        test_data['prediction']=pred_num_list\n",
    "        test_data['score']=score_list\n",
    "        test_data['content']=content\n",
    "        return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table indbakke_fuld created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "res=classification_inference().documentClassification(test_data=Driver_emails().pull_emails(), model_path = 'E:/Scripts/custom ner/Master code/models_foa_5_labels',number_of_words=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Webservice():\n",
    "    def df_to_json(self):\n",
    "                \n",
    "        #ner_output=pd.read_csv('E:/Excel/attachment_text_names_dates.csv', sep=';')\n",
    "        #ner_output=ner_output[pd.notna(ner_output['Text'])]\n",
    "        ner_json=res_ner[['content', 'Names', 'Address', 'Arbejdsgiver', 'Arbejdssted', 'Stilling']]\n",
    "        \n",
    "        text_list=ner_json['content'].values.tolist()\n",
    "        names_list=ner_json['Names'].values.tolist()\n",
    "        address_list=ner_json['Address'].values.tolist()\n",
    "        arbejdsgiver_list=ner_json['Arbejdsgiver'].values.tolist()\n",
    "        arbejdssted_list=ner_json['Arbejdssted'].values.tolist()\n",
    "        stilling_list=ner_json['Stilling'].values.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(len(ner_json)):\n",
    "            text_temp=text_list[i]\n",
    "            names_temp=list(set(names_list[i].split(',')))\n",
    "            address_temp=list(set(address_list[i].split(',')))\n",
    "            arbejdsgiver_temp=list(set(arbejdsgiver_list[i].split(',')))\n",
    "            arbejdssted_temp=list(set(arbejdssted_list[i].split(',')))\n",
    "            stilling_temp=list(set(arbejdssted_list[i].split(',')))\n",
    "        \n",
    "            data_set = {\"Text\": text_temp, \"Navn\": names_temp, \"Address\": address_temp, \"Arbejdsgiver\": arbejdsgiver_temp, \"Arbejdssted\": arbejdssted_temp, \"Stilling\": stilling_temp }\n",
    "            json_dump = json.dumps(data_set, ensure_ascii=False)\n",
    "            \n",
    "            \n",
    "            filename='E:/Web service input/webservice' + str(i+1) + '.json'\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "                json_file.write(json_dump)\n",
    "                \n",
    "            filename='E:/Web service input/webservice' + str(i+1) + '.txt'\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dates:\n",
    "    def is_valid_date_range(self,date):\n",
    "        if(date>1900 and date<=2021):\n",
    "            return True\n",
    "        else: \n",
    "            return False      \n",
    "    \n",
    "    \n",
    "    def extract_dates(self,texts):\n",
    "        dates=[]\n",
    "        # for each entry\n",
    "        for text in texts:\n",
    "            # find the dates\n",
    "            matches = datefinder.find_dates(text)\n",
    "            try:\n",
    "                extracted_dates=[str(match).split(' ')[0] for match in matches if self.is_valid_date_range(int(str(match).split(' ')[0].split('-')[0]))]\n",
    "            except Exception as e:\n",
    "                extracted_dates=[]   \n",
    "            if(len(extracted_dates)>0):\n",
    "                dates.append(','.join(set(extracted_dates)))\n",
    "            else:\n",
    "                dates.append('NA')\n",
    "        # result dataset        \n",
    "        res=pd.DataFrame(columns=['Dates'])  \n",
    "        #res['Text']=texts\n",
    "        res['Dates']=dates\n",
    "        return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_ner:\n",
    "    def predict(self,nlp,data,interested_entities=['Address','Names']):\n",
    "        entities=[]\n",
    "        for text in data:\n",
    "            doc=nlp(text)\n",
    "            temp={}\n",
    "            for en in interested_entities:\n",
    "                temp[en]=[' ']\n",
    "            for ent in doc.ents:\n",
    "                    temp[ent.label_].append(ent.text)  \n",
    "            entities.append(temp)  \n",
    "        return entities\n",
    "\n",
    "    def run_custom_email_ner_da(self,test_data='foadk.xls',content_column='Brødtekst',model_path='custom_email_ner_da_2705.zip',interested_entities=['Address','Names']):\n",
    "        #test_data=pd.read_csv(test_data)\n",
    "        test_data=list(test_data[content_column])\n",
    "        test_data=[str(en) for en in test_data]\n",
    "        # UnZip the custom NER model\n",
    "        with zipfile.ZipFile(model_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('')\n",
    "        # Load the model\n",
    "        mod=spacy.load('Model')  \n",
    "        entities=self.predict(mod,test_data,interested_entities)\n",
    "        res=pd.DataFrame(entities)  \n",
    "        for col in res.columns:\n",
    "            temp=[]\n",
    "            for en in res[col]:\n",
    "                if(en[0]==' '):\n",
    "                    en.pop(0)\n",
    "                temp.append(\",\".join(en))  \n",
    "            res[col]=temp \n",
    "        res['Text']= test_data        \n",
    "        return res\n",
    "    \n",
    "    def extract_cvr(self,data,column_name):\n",
    "        nlp=spacy.load('da_core_news_lg')\n",
    "        cvr_str=[]\n",
    "        texts=[]\n",
    "\n",
    "        for text in data[column_name]:\n",
    "            cvr=UID().extract_cvr(text,nlp)\n",
    "            if(len(cvr)>0):\n",
    "                cvr=list(set(cvr)) \n",
    "            cvr_str.append(','.join(cvr)) \n",
    "            texts.append(text)\n",
    "        temp=pd.DataFrame(columns=['Text','CVR'])\n",
    "        temp['Text']=texts\n",
    "        temp['CVR']=cvr_str\n",
    "        return temp\n",
    "\n",
    "    def extract_cpr(self,data,column_name):\n",
    "        nlp=spacy.load('da_core_news_lg')\n",
    "        cpr_str=[]\n",
    "        texts=[]\n",
    "\n",
    "        for text in data[column_name]:\n",
    "            cpr=UID().extract_cpr(text,nlp)\n",
    "            if(len(cpr)>0):\n",
    "                cpr=list(set(cpr)) \n",
    "            cpr_str.append(','.join(cpr)) \n",
    "            texts.append(text)\n",
    "        temp=pd.DataFrame(columns=['Text','CPR'])\n",
    "        temp['Text']=texts\n",
    "        temp['CPR']=cpr_str\n",
    "        return temp\n",
    "    \n",
    "    def extract_fodselsnummer(self,data,column_name):\n",
    "        nlp=spacy.load('nb_core_news_sm')\n",
    "        fodselsnummer_str=[]\n",
    "        texts=[]\n",
    "\n",
    "        for text in data[column_name]:\n",
    "            fodselsnummer=UID().extract_fodselsnummer(text,nlp)\n",
    "            if(len(fodselsnummer)>0):\n",
    "                fodselsnummer=list(set(fodselsnummer)) \n",
    "            fodselsnummer_str.append(','.join(fodselsnummer)) \n",
    "            texts.append(text)\n",
    "        temp=pd.DataFrame(columns=['Text','Fodselsnummer'])\n",
    "        temp['Text']=texts\n",
    "        temp['Fodselsnummer']=cpr_Fodselsnummer\n",
    "        return temp\n",
    "\n",
    "    def evaluate(self,nlp, data):\n",
    "        scorer = Scorer()\n",
    "        examples = []\n",
    "        for input_, annot in data:\n",
    "            examples.append(Example.from_dict(nlp(input_), annot))\n",
    "        score=scorer.score(examples)\n",
    "        res={}\n",
    "        res['Overall Precision']=score['ents_p']\n",
    "        res['Overall Recall']=score['ents_r']\n",
    "        res['Overall F1-Score']=score['ents_f']\n",
    "        # https://stackoverflow.com/questions/52856057/is-there-a-way-with-spacys-ner-to-calculate-metrics-per-entity-type\n",
    "        x = PrettyTable()\n",
    "        ents=list(score['ents_per_type'].keys())\n",
    "        x.field_names = ['Entities','Precision','Recall','F1-Score']\n",
    "        for en in ents:\n",
    "            x.add_row([en,score['ents_per_type'][en]['p'] , score['ents_per_type'][en]['r'], score['ents_per_type'][en]['f']])\n",
    "        print('----NER Evalution----')\n",
    "        for k in res:\n",
    "            print(k,\":\",res[k])  \n",
    "        print('Entity Wise Metrices')  \n",
    "        print(x)  \n",
    "        return res,x\n",
    "\n",
    "class Driver:\n",
    "    def run_email_ner(self,data_path='Email_attachments.csv',content_column='content',inference_model_path = 'E:/Scripts/custom ner/Master code/models_foa_5_labels',number_of_words=500 ):  \n",
    "        #data=pd.read_csv(data_path)\n",
    "        data=data_path\n",
    "        interested_entities_1=['Address','Names']\n",
    "        interested_entities_2=['Arbejdsgiver','Arbejdssted','Stilling']\n",
    "        res_inf=classification_inference().documentClassification(test_data=data,model_path=inference_model_path,number_of_words=number_of_words,content_column=content_column)\n",
    "        res_inf=res_inf[res_inf.prediction.isin([0,1,2,3])]\n",
    "        res_ner_1=custom_ner().run_custom_email_ner_da(test_data=res_inf,model_path='E:/Scripts/custom ner/Master code/JULY 19/custom_model_1.zip',content_column=content_column,interested_entities=interested_entities_1)\n",
    "        res_ner_2=custom_ner().run_custom_email_ner_da(test_data=res_inf,model_path='E:/Scripts/custom ner/Master code/JULY 19/custom_model_2.zip',content_column=content_column,interested_entities=interested_entities_2)\n",
    "        res_cvr=custom_ner().extract_cvr(res_inf,content_column)\n",
    "        res_cpr=custom_ner().extract_cpr(res_inf,content_column)\n",
    "        res_ner = pd.concat([res_ner_1, res_ner_2], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_inf, res_ner], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_ner, res_cvr], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_ner, res_cpr], axis=1, join=\"inner\")\n",
    "        res_ner = res_ner.loc[:,~res_ner.columns.duplicated()]\n",
    "        res_ner['Dates']=dates().extract_dates(res_ner['Received'].values)\n",
    "        #ner_json=Webservice().df_to_json()\n",
    "        # Uncomment this if we only need the columns which have some values\n",
    "        #res_ner=res_ner[(res_ner['Address']!='')| (res_ner['Names']!='') ]\n",
    "        return res_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_days={1:31,2:28,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}\n",
    "class UID:\n",
    "    # function to check if the year is leap year\n",
    "    def is_leap_year(self,year):\n",
    "        if (year % 4) == 0:\n",
    "            if (year % 100) == 0:\n",
    "                if (year % 400) == 0:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                 return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Utility to validate CPR\n",
    "    def is_valid_cpr(self,input):\n",
    "        res=True\n",
    "        # if dd or mm is 0 return false\n",
    "        if(int(input[0:2])==0 or int(input[2:4])==0):\n",
    "            return False \n",
    "        # if mm > 12 return false     \n",
    "        elif(int(input[2:4])>12):\n",
    "            return False\n",
    "        # If mm is not feb and dd> max no of days in that month return false    \n",
    "        elif(int(input[2:4])!=2 and int(input[0:2])>max_days[int(input[2:4])]):\n",
    "            return False\n",
    "        # If mm is feb    \n",
    "        elif(int(input[2:4])==2):\n",
    "            # if yy<=20 we assume DOB is after 2000\n",
    "            if(int(input[4:6])<=20):\n",
    "                # if yy is leap year, dd>29 return false\n",
    "                if(self.is_leap_year(int('20'+input[4:6]))):\n",
    "                    if(int(input[0:2])>29):\n",
    "                        return False    \n",
    "                # if yy is not leap year, dd>28 return false        \n",
    "                else:\n",
    "                    if(int(input[0:2])>28):\n",
    "                        return False \n",
    "            # if yy>20 DOB is in 19's                \n",
    "            else:\n",
    "                # if yy is leap year, dd>29 return false\n",
    "                if(self.is_leap_year(int('19'+input[4:6]))):\n",
    "                    if(int(input[0:2])>29):\n",
    "                        return False\n",
    "                # if yy is not leap year, dd>28 return false        \n",
    "                else:\n",
    "                    if(int(input[0:2])>28):\n",
    "                        return False                                                      \n",
    "        return res                \n",
    "\n",
    "    # function for CPR pattern matching\n",
    "    def extract_cpr(self,text,model):\n",
    "        text=text.replace('.',\" \")\n",
    "        text=text.replace(',',\" \")\n",
    "        nlp=model\n",
    "        cpr=[]\n",
    "        matcher = Matcher(nlp.vocab) \n",
    "        # Pattern for CPR Matcher\n",
    "        pattern_1 = [{\"TEXT\": {\"REGEX\": \"^\\d{6}\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"-\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"\\d{4}$\"}}]\n",
    "        pattern_2 = [{\"TEXT\": {\"REGEX\": \"^[0-9]{10}$\"}}]       \n",
    "        matcher.add(\"CPR_1\", [pattern_1])\n",
    "        matcher.add(\"CPR_2\", [pattern_2])\n",
    "        doc=nlp(text)  \n",
    "        matches = matcher(doc)\n",
    "        temp=[]\n",
    "        for match_id, start, end in matches:\n",
    "            string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "            span = doc[start:end]\n",
    "            if(len(span.text)>=10 and len(span.text)<12 and self.is_valid_cpr(span.text[0:6])):\n",
    "                temp.append(span.text)\n",
    "        if(len(temp)>0):\n",
    "            return temp\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # DANISH CVR - https://www.wikidata.org/wiki/Property:P1059\n",
    "    def extract_cvr(self,text,model):\n",
    "        nlp = model\n",
    "        matcher = Matcher(nlp.vocab) \n",
    "        # Pattern for danish CVR Matcher\n",
    "        pattern_1= [{\"TEXT\": {\"REGEX\": \"^\\d{8}\"}}]\n",
    "        matcher.add(\"CVR\", [pattern_1])   \n",
    "        doc=nlp(text)  \n",
    "        matches = matcher(doc)\n",
    "        temp=[]\n",
    "        for match_id, start, end in matches:\n",
    "            string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "            span = doc[start:end]\n",
    "            # Validate the extracted patterns using stdnum\n",
    "            if(cvr.is_valid(span.text)):\n",
    "                temp.append(span.text)\n",
    "        if(len(temp)>0):\n",
    "            return temp\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/National_identification_number#Norway\n",
    "    def extract_fodselsnummer(self,text,model):\n",
    "        text=text.replace('.',\" \")\n",
    "        text=text.replace(',',\" \")\n",
    "        nlp=model\n",
    "        cpr=[]\n",
    "        matcher = Matcher(nlp.vocab) \n",
    "        # Pattern for CPR Matcher\n",
    "        pattern_1 = [{\"TEXT\": {\"REGEX\": \"^\\d{6}\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"-\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"\\d{5}$\"}}]\n",
    "        pattern_2 = [{\"TEXT\": {\"REGEX\": \"^\\d{11}\"}}]\n",
    "        pattern_3 = [{\"TEXT\": {\"REGEX\": \"^[0-9]{11}$\"}}]\n",
    "        matcher.add(\"Fodselsnummer_1\", [pattern_1])\n",
    "        matcher.add(\"Fodselsnummer_2\", [pattern_2])\n",
    "        matcher.add(\"Fodselsnummer_3\", [pattern_2])\n",
    "        doc=nlp(text)  \n",
    "        matches = matcher(doc)\n",
    "        temp=[]\n",
    "        for match_id, start, end in matches:\n",
    "            string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "            span = doc[start:end]\n",
    "            # Validate the extracted patterns using stdnum\n",
    "            if(fodselsnummer.is_valid(span.text)):\n",
    "                temp.append(span.text)\n",
    "        if(len(temp)>0):\n",
    "            return temp\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.49it/s]\n",
      "UserWarning: [W095] Model 'da_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate [util.py:730]\n"
     ]
    }
   ],
   "source": [
    "res_ner=Driver().run_email_ner(data_path=Driver_emails().pull_emails(),content_column='Body',inference_model_path = 'E:/Scripts/custom ner/Master code/models_foa_5_labels',number_of_words=500 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Afsender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Body</th>\n",
       "      <th>Received_UTC</th>\n",
       "      <th>Received</th>\n",
       "      <th>Attachment_text</th>\n",
       "      <th>Num_attachments</th>\n",
       "      <th>Message_ID</th>\n",
       "      <th>References</th>\n",
       "      <th>...</th>\n",
       "      <th>content</th>\n",
       "      <th>Address</th>\n",
       "      <th>Names</th>\n",
       "      <th>Text</th>\n",
       "      <th>Arbejdsgiver</th>\n",
       "      <th>Arbejdssted</th>\n",
       "      <th>Stilling</th>\n",
       "      <th>CVR</th>\n",
       "      <th>CPR</th>\n",
       "      <th>Dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indgaaende@prod.e-boks.dk</td>\n",
       "      <td>Varde Kommune</td>\n",
       "      <td>Meddelelser fra Varde Kommune Opsigelse, Varde...</td>\n",
       "      <td>Email template Afsender: Varde Kommune Referen...</td>\n",
       "      <td>2021-07-21 07:01:26+00:00</td>\n",
       "      <td>2021-07-21 09:01:26+00:00</td>\n",
       "      <td>Løn og Forhandling Bytoften 2 6800 Varde Tlf....</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;459615552.39.1626850885686@cis-drift1.foa.dk&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Email template Afsender: Varde Kommune Referen...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Tak – og også til dig J Med venlig hilsen Katr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kabla@FOA.DK</td>\n",
       "      <td></td>\n",
       "      <td>Ikke på kontoret (fast ordning om fysisk fremm...</td>\n",
       "      <td>Hej med jer, Jeg er ikke på kontoret i dag og ...</td>\n",
       "      <td>2021-07-21 07:06:16+00:00</td>\n",
       "      <td>2021-07-21 09:06:16+00:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;754c83ec381840f6afe7029786f28be7@FOA.DK&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Hej med jer, Jeg er ikke på kontoret i dag og ...</td>\n",
       "      <td></td>\n",
       "      <td>Lene Schultz Personalefuldmægtig</td>\n",
       "      <td>Jeg sender vedlagt et forhandlingsreferat vedr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kabla@FOA.DK</td>\n",
       "      <td></td>\n",
       "      <td>SV: Ikke på kontoret (fast ordning om fysisk f...</td>\n",
       "      <td>Tak – og også til dig J Med venlig hilsen Katr...</td>\n",
       "      <td>2021-07-21 07:13:41+00:00</td>\n",
       "      <td>2021-07-21 09:13:41+00:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;96f4c2fa49294db2b6ed038761d07ef7@FOA.DK&gt;</td>\n",
       "      <td>&lt;754c83ec381840f6afe7029786f28be7@FOA.DK&gt; &lt;e4e...</td>\n",
       "      <td>...</td>\n",
       "      <td>Tak – og også til dig J Med venlig hilsen Katr...</td>\n",
       "      <td>Frederiksværksgade 10, DK 3400 Hillerød</td>\n",
       "      <td>Monia Stoltz</td>\n",
       "      <td>Kære FOA Overenskomst Jeg har fået en henvende...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kommunen@horsholm.dk</td>\n",
       "      <td></td>\n",
       "      <td>{{forha001}}Lønforhandling</td>\n",
       "      <td>Jeg sender vedlagt et forhandlingsreferat vedr...</td>\n",
       "      <td>2021-07-21 07:21:03+00:00</td>\n",
       "      <td>2021-07-21 09:21:03+00:00</td>\n",
       "      <td>BREELTEPARKEN Selvejende Institution Protekt...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;1588252074.389850.1626852061394.JavaMail.TITA...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Jeg sender vedlagt et forhandlingsreferat vedr...</td>\n",
       "      <td>Frederiksværksgade 10, DK 3400 Hillerød</td>\n",
       "      <td></td>\n",
       "      <td>Kære FOA Overenskomst Jeg har fået en henvende...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Sender       Afsender  \\\n",
       "2  indgaaende@prod.e-boks.dk  Varde Kommune   \n",
       "3               kabla@FOA.DK                  \n",
       "4               kabla@FOA.DK                  \n",
       "5       kommunen@horsholm.dk                  \n",
       "\n",
       "                                             Subject  \\\n",
       "2  Meddelelser fra Varde Kommune Opsigelse, Varde...   \n",
       "3  Ikke på kontoret (fast ordning om fysisk fremm...   \n",
       "4  SV: Ikke på kontoret (fast ordning om fysisk f...   \n",
       "5                         {{forha001}}Lønforhandling   \n",
       "\n",
       "                                                Body  \\\n",
       "2  Email template Afsender: Varde Kommune Referen...   \n",
       "3  Hej med jer, Jeg er ikke på kontoret i dag og ...   \n",
       "4  Tak – og også til dig J Med venlig hilsen Katr...   \n",
       "5  Jeg sender vedlagt et forhandlingsreferat vedr...   \n",
       "\n",
       "               Received_UTC                  Received  \\\n",
       "2 2021-07-21 07:01:26+00:00 2021-07-21 09:01:26+00:00   \n",
       "3 2021-07-21 07:06:16+00:00 2021-07-21 09:06:16+00:00   \n",
       "4 2021-07-21 07:13:41+00:00 2021-07-21 09:13:41+00:00   \n",
       "5 2021-07-21 07:21:03+00:00 2021-07-21 09:21:03+00:00   \n",
       "\n",
       "                                     Attachment_text  Num_attachments  \\\n",
       "2   Løn og Forhandling Bytoften 2 6800 Varde Tlf....                1   \n",
       "3                                                                   0   \n",
       "4                                                                   0   \n",
       "5    BREELTEPARKEN Selvejende Institution Protekt...                1   \n",
       "\n",
       "                                          Message_ID  \\\n",
       "2     <459615552.39.1626850885686@cis-drift1.foa.dk>   \n",
       "3          <754c83ec381840f6afe7029786f28be7@FOA.DK>   \n",
       "4          <96f4c2fa49294db2b6ed038761d07ef7@FOA.DK>   \n",
       "5  <1588252074.389850.1626852061394.JavaMail.TITA...   \n",
       "\n",
       "                                          References  ...  \\\n",
       "2                                               None  ...   \n",
       "3                                               None  ...   \n",
       "4  <754c83ec381840f6afe7029786f28be7@FOA.DK> <e4e...  ...   \n",
       "5                                               None  ...   \n",
       "\n",
       "                                             content  \\\n",
       "2  Email template Afsender: Varde Kommune Referen...   \n",
       "3  Hej med jer, Jeg er ikke på kontoret i dag og ...   \n",
       "4  Tak – og også til dig J Med venlig hilsen Katr...   \n",
       "5  Jeg sender vedlagt et forhandlingsreferat vedr...   \n",
       "\n",
       "                                   Address                             Names  \\\n",
       "2                                                                              \n",
       "3                                           Lene Schultz Personalefuldmægtig   \n",
       "4  Frederiksværksgade 10, DK 3400 Hillerød                      Monia Stoltz   \n",
       "5  Frederiksværksgade 10, DK 3400 Hillerød                                     \n",
       "\n",
       "                                                Text Arbejdsgiver Arbejdssted  \\\n",
       "2  Tak – og også til dig J Med venlig hilsen Katr...                            \n",
       "3  Jeg sender vedlagt et forhandlingsreferat vedr...                            \n",
       "4  Kære FOA Overenskomst Jeg har fået en henvende...                            \n",
       "5  Kære FOA Overenskomst Jeg har fået en henvende...                            \n",
       "\n",
       "  Stilling CVR CPR Dates  \n",
       "2                     NA  \n",
       "3                     NA  \n",
       "4                    NaN  \n",
       "5                    NaN  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n=get_classification_data_in_chunks(data.Attachment_text,data.Afskedigelse)\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    # method to return the text data as chunks based on treshold\n",
    "    def get_chunks(self,x='FOA - Fag og Arbejde Staunings Plads 1 1607',threshold=7):\n",
    "        temp=[]\n",
    "        # total no of chunks\n",
    "        num_chunks=math.ceil(len(x)/threshold)\n",
    "        # create the chunks\n",
    "        for i in range(num_chunks):\n",
    "            if(i==0):\n",
    "                temp.append(x[i:i+threshold])  \n",
    "            elif(i>0 and i+threshold<len(x)):\n",
    "                temp.append(x[i*threshold:2*i*threshold])              \n",
    "        return  temp\n",
    "    # method to get x,y for classification as chunks\n",
    "    def get_classification_data_in_chunks(self,x,y,threshold=400,max_length=1000):\n",
    "        x_=[]\n",
    "        y_=[]\n",
    "        for i in range(len(x)):\n",
    "            x[i]=str(x[i])\n",
    "            x[i]=x[i][0:max_length]\n",
    "            if(len(x[i])>threshold):\n",
    "                temp_x=self.get_chunks(x[i],threshold)\n",
    "                temp_y=[y[i] for j in range(len(temp_x))]\n",
    "                x_+=temp_x\n",
    "                y_+=temp_y\n",
    "            else:\n",
    "                x_.append(x[i])\n",
    "                y_.append(y[i])     \n",
    "        return x_,y_                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Driver:\n",
    "    def run_email_ner(self,data_path='Email_attachments.csv',content_column='content',inference_model_path = 'E:/Scripts/custom ner/Master code/models_foa_5_labels',number_of_words=500, threshold=300 ):  \n",
    "        #data=pd.read_csv(data_path)\n",
    "        data=data_path\n",
    "        x=data[content_column]\n",
    "        x=[str(en) for en in x]\n",
    "        x=[en[0:threshold] for en in x]\n",
    "        list_chunks=Util().get_chunks(x,threshold)\n",
    "        df_chunks=pd.DataFrame(columns=[content_column])\n",
    "        df_chunks[content_column]=list_chunks\n",
    "        interested_entities_1=['Address','Names']\n",
    "        interested_entities_2=['Arbejdsgiver','Arbejdssted','Stilling']\n",
    "        res_inf=classification_inference().documentClassification(test_data=df_chunks,model_path=inference_model_path,number_of_words=number_of_words,content_column=content_column)\n",
    "        res_inf=res_inf[res_inf.prediction.isin([0,1,2,3])]\n",
    "        res_ner_1=custom_ner().run_custom_email_ner_da(test_data=res_inf,model_path='E:/Scripts/custom ner/Master code/JULY 19/custom_model_1.zip',content_column=content_column,interested_entities=interested_entities_1)\n",
    "        res_ner_2=custom_ner().run_custom_email_ner_da(test_data=res_inf,model_path='E:/Scripts/custom ner/Master code/JULY 19/custom_model_2.zip',content_column=content_column,interested_entities=interested_entities_2)\n",
    "        res_cvr=custom_ner().extract_cvr(res_inf,content_column)\n",
    "        res_cpr=custom_ner().extract_cpr(res_inf,content_column)\n",
    "        res_ner = pd.concat([res_ner_1, res_ner_2], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_inf, res_ner], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_ner, res_cvr], axis=1, join=\"inner\")\n",
    "        res_ner = pd.concat([res_ner, res_cpr], axis=1, join=\"inner\")\n",
    "        res_ner = res_ner.loc[:,~res_ner.columns.duplicated()]\n",
    "        res_ner['Dates']=dates().extract_dates(res_ner['Received'].values)\n",
    "        #ner_json=Webservice().df_to_json()\n",
    "        # Uncomment this if we only need the columns which have some values\n",
    "        #res_ner=res_ner[(res_ner['Address']!='')| (res_ner['Names']!='') ]\n",
    "        return res_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
